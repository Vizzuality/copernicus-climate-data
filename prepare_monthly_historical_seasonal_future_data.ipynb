{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepare_monthly_historical_seasonal_future_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1gIc0joNKnmkZYCtQiLOWH48zbv6zSNY-",
      "authorship_tag": "ABX9TyN52EinbxaWvTAvlx/3yHeU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vizzuality/copernicus-climate-data/blob/master/prepare_monthly_historical_seasonal_future_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTLRdjsHkjtl",
        "colab_type": "text"
      },
      "source": [
        "# Prepare data for the copernicus-climate project\n",
        "\n",
        "https://github.com/Vizzuality/copernicus-climate-data\n",
        "\n",
        "`Edward P. Morris (vizzuality.)`\n",
        "\n",
        "## Description\n",
        "This notebook creates monthly historical summaries for maximum and minimum `pet`, surface temperature, coldsnaps and heatwaves. Future seasonal and longterm summaries are also created. All data arrays for each theme are grouped and written to GCS as Zarr groups for further processing.  \n",
        "\n",
        "## TODO\n",
        "+ check dtypes and convert to simpler types; float64 to float32 reduces array size by 2. see https://xarray.pydata.org/en/stable/io.html#writing-encoded-data\n",
        "+ Transpose so order is min to max variation\n",
        "```\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2020 Vizzuality\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKARM1_f3Ccg",
        "colab_type": "text"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Instructions for setting up the computing environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f327xg8JujU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "033609a4-047f-4d6f-a2e6-81eef1172bcc"
      },
      "source": [
        "%%bash\n",
        "# Remove sample_data\n",
        "rm -r sample_data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'sample_data': No such file or directory\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjdjX-Vc3LVK",
        "colab_type": "text"
      },
      "source": [
        "## Linux dependencies\n",
        "\n",
        "Instructions for adding linux (including node, ect.) system packages.\n",
        "\n",
        "``` \n",
        "!apt install -q -y <package-name>\n",
        "!npm install -g <package-name>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcfGoVu97jr2",
        "colab_type": "code",
        "outputId": "81f1c618-595f-4c64-beca-23a762d3883e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Packages for projections and geospatial processing\n",
        "!apt install -q -y libspatialindex-dev libproj-dev proj-data proj-bin libgeos-dev"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libgeos-dev is already the newest version (3.6.2-1build2).\n",
            "libproj-dev is already the newest version (4.9.3-2).\n",
            "libspatialindex-dev is already the newest version (1.8.5-5).\n",
            "proj-bin is already the newest version (4.9.3-2).\n",
            "proj-data is already the newest version (4.9.3-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uXsnPYfI65ot"
      },
      "source": [
        "## Python packages\n",
        "\n",
        "Consider using package versions to ensure nothing changes.\n",
        "\n",
        "`!pip install -q <package-name>`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BNK1kif6VXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# connect to Google cloud storage\n",
        "!pip install -q gcsfs "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MelOBk55M8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# geospatial tools\n",
        "!pip install -q country-converter rtree geopandas shapely fiona"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-PyPT2y57gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# netcdf, xarray, xclim, and Zarr tools\n",
        "!pip install -q cftime netcdf4 nc-time-axis zarr xarray xclim rioxarray regionmask sparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s7-DYbxjesM",
        "colab_type": "code",
        "outputId": "d3095b94-956e-4fb0-e933-80510ac73f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Show python package versions\n",
        "!pip list"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Package                  Version        \n",
            "------------------------ ---------------\n",
            "absl-py                  0.9.0          \n",
            "affine                   2.3.0          \n",
            "alabaster                0.7.12         \n",
            "albumentations           0.1.12         \n",
            "altair                   4.1.0          \n",
            "asciitree                0.3.3          \n",
            "asgiref                  3.2.7          \n",
            "astor                    0.8.1          \n",
            "astropy                  4.0.1.post1    \n",
            "astunparse               1.6.3          \n",
            "atari-py                 0.2.6          \n",
            "atomicwrites             1.3.0          \n",
            "attrs                    19.3.0         \n",
            "audioread                2.1.8          \n",
            "autograd                 1.3            \n",
            "Babel                    2.8.0          \n",
            "backcall                 0.1.0          \n",
            "beautifulsoup4           4.6.3          \n",
            "bleach                   3.1.4          \n",
            "blis                     0.4.1          \n",
            "bokeh                    1.4.0          \n",
            "boltons                  20.1.0         \n",
            "boto                     2.49.0         \n",
            "boto3                    1.12.47        \n",
            "botocore                 1.15.47        \n",
            "Bottleneck               1.3.2          \n",
            "branca                   0.4.0          \n",
            "bs4                      0.0.1          \n",
            "CacheControl             0.12.6         \n",
            "cachetools               3.1.1          \n",
            "Cartopy                  0.18.0         \n",
            "catalogue                1.0.0          \n",
            "certifi                  2020.4.5.1     \n",
            "cffi                     1.14.0         \n",
            "cftime                   1.1.2          \n",
            "chainer                  6.5.0          \n",
            "chardet                  3.0.4          \n",
            "click                    7.1.2          \n",
            "click-plugins            1.1.1          \n",
            "cligj                    0.5.0          \n",
            "cloudpickle              1.3.0          \n",
            "cmake                    3.12.0         \n",
            "cmdstanpy                0.4.0          \n",
            "colorlover               0.3.0          \n",
            "community                1.0.0b1        \n",
            "contextlib2              0.5.5          \n",
            "convertdate              2.2.0          \n",
            "country-converter        0.6.7          \n",
            "coverage                 3.7.1          \n",
            "coveralls                0.5            \n",
            "crcmod                   1.7            \n",
            "cufflinks                0.17.3         \n",
            "cvxopt                   1.2.5          \n",
            "cvxpy                    1.0.31         \n",
            "cycler                   0.10.0         \n",
            "cymem                    2.0.3          \n",
            "Cython                   0.29.17        \n",
            "daft                     0.0.4          \n",
            "dask                     2.12.0         \n",
            "dataclasses              0.7            \n",
            "datascience              0.10.6         \n",
            "decorator                4.4.2          \n",
            "defusedxml               0.6.0          \n",
            "descartes                1.1.0          \n",
            "dill                     0.3.1.1        \n",
            "distributed              2.15.2         \n",
            "Django                   3.0.5          \n",
            "dlib                     19.18.0        \n",
            "docopt                   0.6.2          \n",
            "docutils                 0.15.2         \n",
            "dopamine-rl              1.0.5          \n",
            "earthengine-api          0.1.219        \n",
            "easydict                 1.9            \n",
            "ecos                     2.0.7.post1    \n",
            "editdistance             0.5.3          \n",
            "en-core-web-sm           2.2.5          \n",
            "entrypoints              0.3            \n",
            "ephem                    3.7.7.1        \n",
            "et-xmlfile               1.0.1          \n",
            "fa2                      0.3.5          \n",
            "fancyimpute              0.4.3          \n",
            "fastai                   1.0.60         \n",
            "fastdtw                  0.3.4          \n",
            "fasteners                0.15           \n",
            "fastprogress             0.2.3          \n",
            "fastrlock                0.4            \n",
            "fbprophet                0.6            \n",
            "feather-format           0.4.1          \n",
            "featuretools             0.4.1          \n",
            "filelock                 3.0.12         \n",
            "Fiona                    1.8.13.post1   \n",
            "firebase-admin           4.1.0          \n",
            "fix-yahoo-finance        0.0.22         \n",
            "Flask                    1.1.2          \n",
            "folium                   0.8.3          \n",
            "fsspec                   0.7.3          \n",
            "future                   0.16.0         \n",
            "gast                     0.3.3          \n",
            "gcsfs                    0.6.1          \n",
            "GDAL                     2.2.2          \n",
            "gdown                    3.6.4          \n",
            "gensim                   3.6.0          \n",
            "geographiclib            1.50           \n",
            "geojson                  2.5.0          \n",
            "geopandas                0.7.0          \n",
            "geopy                    1.17.0         \n",
            "gin-config               0.3.0          \n",
            "glob2                    0.7            \n",
            "google                   2.0.3          \n",
            "google-api-core          1.16.0         \n",
            "google-api-python-client 1.7.12         \n",
            "google-auth              1.7.2          \n",
            "google-auth-httplib2     0.0.3          \n",
            "google-auth-oauthlib     0.4.1          \n",
            "google-cloud-bigquery    1.21.0         \n",
            "google-cloud-core        1.0.3          \n",
            "google-cloud-datastore   1.8.0          \n",
            "google-cloud-firestore   1.6.2          \n",
            "google-cloud-language    1.2.0          \n",
            "google-cloud-storage     1.18.1         \n",
            "google-cloud-translate   1.5.0          \n",
            "google-colab             1.0.0          \n",
            "google-pasta             0.2.0          \n",
            "google-resumable-media   0.4.1          \n",
            "googleapis-common-protos 1.51.0         \n",
            "googledrivedownloader    0.4            \n",
            "graphviz                 0.10.1         \n",
            "grpcio                   1.28.1         \n",
            "gspread                  3.0.1          \n",
            "gspread-dataframe        3.0.6          \n",
            "gym                      0.17.1         \n",
            "h5py                     2.10.0         \n",
            "HeapDict                 1.0.1          \n",
            "holidays                 0.9.12         \n",
            "html5lib                 1.0.1          \n",
            "httpimport               0.5.18         \n",
            "httplib2                 0.17.3         \n",
            "httplib2shim             0.0.3          \n",
            "humanize                 0.5.1          \n",
            "hyperopt                 0.1.2          \n",
            "ideep4py                 2.0.0.post3    \n",
            "idna                     2.9            \n",
            "image                    1.5.31         \n",
            "imageio                  2.4.1          \n",
            "imagesize                1.2.0          \n",
            "imbalanced-learn         0.4.3          \n",
            "imblearn                 0.0            \n",
            "imgaug                   0.2.9          \n",
            "importlib-metadata       1.6.0          \n",
            "imutils                  0.5.3          \n",
            "inflect                  2.1.0          \n",
            "intel-openmp             2020.0.133     \n",
            "intervaltree             2.1.0          \n",
            "ipykernel                4.10.1         \n",
            "ipython                  5.5.0          \n",
            "ipython-genutils         0.2.0          \n",
            "ipython-sql              0.3.9          \n",
            "ipywidgets               7.5.1          \n",
            "itsdangerous             1.1.0          \n",
            "jax                      0.1.64         \n",
            "jaxlib                   0.1.45         \n",
            "jdcal                    1.4.1          \n",
            "jedi                     0.17.0         \n",
            "jieba                    0.42.1         \n",
            "Jinja2                   2.11.2         \n",
            "jmespath                 0.9.5          \n",
            "joblib                   0.14.1         \n",
            "jpeg4py                  0.1.4          \n",
            "jsonschema               2.6.0          \n",
            "jupyter                  1.0.0          \n",
            "jupyter-client           5.3.4          \n",
            "jupyter-console          5.2.0          \n",
            "jupyter-core             4.6.3          \n",
            "kaggle                   1.5.6          \n",
            "kapre                    0.1.3.1        \n",
            "Keras                    2.3.1          \n",
            "Keras-Applications       1.0.8          \n",
            "Keras-Preprocessing      1.1.0          \n",
            "keras-vis                0.4.1          \n",
            "kiwisolver               1.2.0          \n",
            "knnimpute                0.1.0          \n",
            "librosa                  0.6.3          \n",
            "lightgbm                 2.2.3          \n",
            "llvmlite                 0.31.0         \n",
            "lmdb                     0.98           \n",
            "locket                   0.2.0          \n",
            "lucid                    0.3.8          \n",
            "LunarCalendar            0.0.9          \n",
            "lxml                     4.2.6          \n",
            "Markdown                 3.2.1          \n",
            "MarkupSafe               1.1.1          \n",
            "matplotlib               3.2.1          \n",
            "matplotlib-venn          0.11.5         \n",
            "missingno                0.4.2          \n",
            "mistune                  0.8.4          \n",
            "mizani                   0.6.0          \n",
            "mkl                      2019.0         \n",
            "mlxtend                  0.14.0         \n",
            "monotonic                1.5            \n",
            "more-itertools           8.2.0          \n",
            "moviepy                  0.2.3.5        \n",
            "mpmath                   1.1.0          \n",
            "msgpack                  1.0.0          \n",
            "multiprocess             0.70.9         \n",
            "multitasking             0.0.9          \n",
            "munch                    2.5.0          \n",
            "murmurhash               1.0.2          \n",
            "music21                  5.5.0          \n",
            "natsort                  5.5.0          \n",
            "nbconvert                5.6.1          \n",
            "nbformat                 5.0.6          \n",
            "nc-time-axis             1.2.0          \n",
            "netCDF4                  1.5.3          \n",
            "networkx                 2.4            \n",
            "nibabel                  3.0.2          \n",
            "nltk                     3.2.5          \n",
            "notebook                 5.2.2          \n",
            "np-utils                 0.5.12.1       \n",
            "numba                    0.48.0         \n",
            "numcodecs                0.6.4          \n",
            "numexpr                  2.7.1          \n",
            "numpy                    1.18.3         \n",
            "nvidia-ml-py3            7.352.0        \n",
            "oauth2client             4.1.3          \n",
            "oauthlib                 3.1.0          \n",
            "okgrade                  0.4.3          \n",
            "opencv-contrib-python    4.1.2.30       \n",
            "opencv-python            4.1.2.30       \n",
            "openpyxl                 2.5.9          \n",
            "opt-einsum               3.2.1          \n",
            "osqp                     0.6.1          \n",
            "packaging                20.3           \n",
            "palettable               3.3.0          \n",
            "pandas                   1.0.3          \n",
            "pandas-datareader        0.8.1          \n",
            "pandas-gbq               0.11.0         \n",
            "pandas-profiling         1.4.1          \n",
            "pandocfilters            1.4.2          \n",
            "parso                    0.7.0          \n",
            "partd                    1.1.0          \n",
            "pathlib                  1.0.1          \n",
            "patsy                    0.5.1          \n",
            "pexpect                  4.8.0          \n",
            "pickleshare              0.7.5          \n",
            "Pillow                   7.0.0          \n",
            "Pint                     0.11           \n",
            "pip                      19.3.1         \n",
            "pip-tools                4.5.1          \n",
            "plac                     1.1.3          \n",
            "plotly                   4.4.1          \n",
            "plotnine                 0.6.0          \n",
            "pluggy                   0.7.1          \n",
            "portpicker               1.3.1          \n",
            "prefetch-generator       1.0.1          \n",
            "preshed                  3.0.2          \n",
            "prettytable              0.7.2          \n",
            "progressbar2             3.38.0         \n",
            "prometheus-client        0.7.1          \n",
            "promise                  2.3            \n",
            "prompt-toolkit           1.0.18         \n",
            "protobuf                 3.10.0         \n",
            "psutil                   5.4.8          \n",
            "psycopg2                 2.7.6.1        \n",
            "ptvsd                    5.0.0a12       \n",
            "ptyprocess               0.6.0          \n",
            "py                       1.8.1          \n",
            "pyarrow                  0.14.1         \n",
            "pyasn1                   0.4.8          \n",
            "pyasn1-modules           0.2.8          \n",
            "pycocotools              2.0.0          \n",
            "pycparser                2.20           \n",
            "pydata-google-auth       1.1.0          \n",
            "pydot                    1.3.0          \n",
            "pydot-ng                 2.0.0          \n",
            "pydotplus                2.0.2          \n",
            "PyDrive                  1.3.1          \n",
            "pyemd                    0.5.1          \n",
            "pyglet                   1.5.0          \n",
            "Pygments                 2.1.3          \n",
            "pygobject                3.26.1         \n",
            "pymc3                    3.7            \n",
            "PyMeeus                  0.3.7          \n",
            "pymongo                  3.10.1         \n",
            "pymystem3                0.2.0          \n",
            "PyOpenGL                 3.1.5          \n",
            "pyparsing                2.4.7          \n",
            "pyproj                   2.6.1          \n",
            "pyrsistent               0.16.0         \n",
            "pyshp                    2.1.0          \n",
            "pysndfile                1.3.8          \n",
            "PySocks                  1.7.1          \n",
            "pystan                   2.19.1.1       \n",
            "pytest                   3.6.4          \n",
            "python-apt               1.6.5+ubuntu0.2\n",
            "python-chess             0.23.11        \n",
            "python-dateutil          2.8.1          \n",
            "python-louvain           0.14           \n",
            "python-slugify           4.0.0          \n",
            "python-utils             2.4.0          \n",
            "pytz                     2018.9         \n",
            "PyWavelets               1.1.1          \n",
            "PyYAML                   3.13           \n",
            "pyzmq                    19.0.0         \n",
            "qtconsole                4.7.3          \n",
            "QtPy                     1.9.0          \n",
            "rasterio                 1.1.3          \n",
            "regex                    2019.12.20     \n",
            "regionmask               0.5.0          \n",
            "requests                 2.23.0         \n",
            "requests-oauthlib        1.3.0          \n",
            "resampy                  0.2.2          \n",
            "retrying                 1.3.3          \n",
            "rioxarray                0.0.26         \n",
            "rpy2                     3.2.7          \n",
            "rsa                      4.0            \n",
            "Rtree                    0.9.4          \n",
            "s3fs                     0.4.2          \n",
            "s3transfer               0.3.3          \n",
            "scikit-image             0.16.2         \n",
            "scikit-learn             0.22.2.post1   \n",
            "scipy                    1.4.1          \n",
            "screen-resolution-extra  0.0.0          \n",
            "scs                      2.1.2          \n",
            "seaborn                  0.10.1         \n",
            "Send2Trash               1.5.0          \n",
            "setuptools               46.1.3         \n",
            "setuptools-git           1.2            \n",
            "Shapely                  1.7.0          \n",
            "simplegeneric            0.8.1          \n",
            "six                      1.12.0         \n",
            "sklearn                  0.0            \n",
            "sklearn-pandas           1.8.0          \n",
            "smart-open               1.11.1         \n",
            "snowballstemmer          2.0.0          \n",
            "snuggs                   1.4.7          \n",
            "sortedcontainers         2.1.0          \n",
            "spacy                    2.2.4          \n",
            "sparse                   0.9.1          \n",
            "Sphinx                   1.8.5          \n",
            "sphinxcontrib-websupport 1.2.1          \n",
            "SQLAlchemy               1.3.16         \n",
            "sqlparse                 0.3.1          \n",
            "srsly                    1.0.2          \n",
            "statsmodels              0.10.2         \n",
            "sympy                    1.1.1          \n",
            "tables                   3.4.4          \n",
            "tabulate                 0.8.7          \n",
            "tbb                      2020.0.133     \n",
            "tblib                    1.6.0          \n",
            "tensorboard              2.2.1          \n",
            "tensorboard-plugin-wit   1.6.0.post3    \n",
            "tensorboardcolab         0.0.22         \n",
            "tensorflow               2.2.0rc3       \n",
            "tensorflow-addons        0.8.3          \n",
            "tensorflow-datasets      2.1.0          \n",
            "tensorflow-estimator     2.2.0          \n",
            "tensorflow-gcs-config    2.1.8          \n",
            "tensorflow-hub           0.8.0          \n",
            "tensorflow-metadata      0.21.2         \n",
            "tensorflow-privacy       0.2.2          \n",
            "tensorflow-probability   0.10.0rc0      \n",
            "termcolor                1.1.0          \n",
            "terminado                0.8.3          \n",
            "testpath                 0.4.4          \n",
            "text-unidecode           1.3            \n",
            "textblob                 0.15.3         \n",
            "textgenrnn               1.4.1          \n",
            "Theano                   1.0.4          \n",
            "thinc                    7.4.0          \n",
            "toolz                    0.10.0         \n",
            "torch                    1.5.0+cu101    \n",
            "torchsummary             1.5.1          \n",
            "torchtext                0.3.1          \n",
            "torchvision              0.6.0+cu101    \n",
            "tornado                  6.0.4          \n",
            "tqdm                     4.38.0         \n",
            "traitlets                4.3.3          \n",
            "tweepy                   3.6.0          \n",
            "typeguard                2.7.1          \n",
            "typing                   3.6.6          \n",
            "typing-extensions        3.6.6          \n",
            "tzlocal                  1.5.1          \n",
            "umap-learn               0.4.1          \n",
            "uritemplate              3.0.1          \n",
            "urllib3                  1.24.3         \n",
            "vega-datasets            0.8.0          \n",
            "wasabi                   0.6.0          \n",
            "wcwidth                  0.1.9          \n",
            "webencodings             0.5.1          \n",
            "Werkzeug                 1.0.1          \n",
            "wheel                    0.34.2         \n",
            "widgetsnbextension       3.5.1          \n",
            "wordcloud                1.5.0          \n",
            "wrapt                    1.12.1         \n",
            "xarray                   0.15.1         \n",
            "xclim                    0.16.0         \n",
            "xgboost                  0.90           \n",
            "xkit                     0.0.0          \n",
            "xlrd                     1.1.0          \n",
            "xlwt                     1.3.0          \n",
            "yellowbrick              0.9.1          \n",
            "zarr                     2.4.0          \n",
            "zict                     2.0.0          \n",
            "zipp                     3.1.0          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK4kYzml3bDD",
        "colab_type": "text"
      },
      "source": [
        "## Authorisation\n",
        "\n",
        "Setting up connections and authorisation to cloud services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7jp5JdFrxGm",
        "colab_type": "text"
      },
      "source": [
        "### Google Cloud\n",
        "\n",
        "This can be done in the URL or via adding service account credentials.\n",
        "\n",
        "If you do not share the notebook, you can mount your Drive and and transfer credentials to disk. Note if the notebook is shared you always need to authenticate via URL.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIeVojTV21Nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Google Cloud information\n",
        "gc_project = \"skydipper-196010\"\n",
        "gc_creds = \"skydipper-196010-f842645fd0f3.json\"\n",
        "gc_user = \"edward-morris@skydipper-196010.iam.gserviceaccount.com\"\n",
        "gcs_prefix = \"gs://copernicus-climate\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oY_ymKY_oIY8",
        "colab": {}
      },
      "source": [
        "# For auth WITHOUT service account\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "#from google.colab import auth\n",
        "#auth.authenticate_user()\n",
        "#!gcloud config set project {project_id}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7akT-7lZ9x3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If the notebook is shared\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dCFS8FOnzScr",
        "colab": {}
      },
      "source": [
        "# If Drive is mounted, copy GC credentials to home (place in your GDrive, and connect Drive)\n",
        "!cp \"/content/drive/My Drive/{gc_creds}\" \"/root/.{gc_creds}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "03Tbqeq9zSc0",
        "colab": {}
      },
      "source": [
        "# Auth WITH service account\n",
        "!gcloud auth activate-service-account {gc_user} --key-file=/root/.{gc_creds} --project={gc_project}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PA4tPZ4-zSc9",
        "colab": {}
      },
      "source": [
        "# Test GC auth\n",
        "!gsutil ls {gcs_prefix}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb3bFXTp0JFz",
        "colab_type": "text"
      },
      "source": [
        "# Utils\n",
        "\n",
        "Generic helper functions used in the subsequent processing. For easy navigation each function seperated into a section with the function name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l-NVtcE8KdV",
        "colab_type": "text"
      },
      "source": [
        "## copy_gcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1KygWllicHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "def copy_gcs(source_list, dest_list, opts=\"\"):\n",
        "  \"\"\"\n",
        "  Use gsutil to copy each corresponding item in source_list\n",
        "  to dest_list.\n",
        "\n",
        "  Example:\n",
        "  copy_gcs([\"gs://my-bucket/data-file.csv\"], [\".\"])\n",
        "\n",
        "  \"\"\"\n",
        "  for s, d  in zip(source_list, dest_list):\n",
        "    cmd = f\"gsutil -m cp -r {opts} {s} {d}\"\n",
        "    print(f\"Processing: {cmd}\")\n",
        "    r = subprocess.call(cmd, shell=True)\n",
        "    if r == 0:\n",
        "        print(\"Task created\")\n",
        "    else:\n",
        "        print(\"Task failed\")\n",
        "  print(\"Finished copy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqlC-HYq8XuR",
        "colab_type": "text"
      },
      "source": [
        "## list_paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-6sC0Zc5HGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import subprocess\n",
        "\n",
        "def list_paths(uri_prefix, dir_path, file_pattern=\"*\", gsutil=True, return_dir_path=True):\n",
        "        ''' Creates a list of full paths \n",
        "    \n",
        "        Uses glob regex rules allowing flexible patterns\n",
        "    \n",
        "        Parameters\n",
        "        ----------\n",
        "        uri_prefix : str\n",
        "            The (GCS) uri prefix.\n",
        "        dir_path : str\n",
        "            Directory path, can use regex.\n",
        "        file_pattern : str\n",
        "            File pattern for glob searching.\n",
        "        gsutil : bool\n",
        "            Use gsutil, default is True.\n",
        "        return_dir_path : bool\n",
        "            Return directory path relative to uri_prefix, default is True.        \n",
        "    \n",
        "        Returns\n",
        "        -------\n",
        "        List of path strings.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        # Requires authentication\n",
        "        #list_paths(\"gs://skydipper-water-quality\", \"cloud-masks/*\", \"*.tif\", True, False)\n",
        "        '''\n",
        "        p = f\"{uri_prefix}/{dir_path}/{file_pattern}\"\n",
        "        print(f\"Searching {p}\")\n",
        "        if not gsutil:\n",
        "          out = glob.glob(p)\n",
        "        if gsutil:\n",
        "          cmd = f\"gsutil ls {p}\"\n",
        "          out = subprocess.check_output(cmd, shell=True).decode('utf8').split('\\n')\n",
        "          out.pop(-1)\n",
        "        if return_dir_path:\n",
        "          out = [f.split(uri_prefix)[1] for f in out]  \n",
        "        print(f\"Found {len(out)} path(s)\")\n",
        "        return out\n",
        "\n",
        "#path_list = list_paths(\"gs://skydipper-water-quality\", \"cloud-masks/*\", \"*.tif\", True, True)\n",
        "#print(path_list[0])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdQybgKyDwnS",
        "colab_type": "text"
      },
      "source": [
        "## mkdirs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UJ1hdijD8iD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def mkdirs(dirs_list, exist_ok=True):\n",
        "  \"\"\" Create nested directories\n",
        "  \"\"\"\n",
        "  for p in dirs_list:\n",
        "    Path(p).mkdir(parents=True, exist_ok=exist_ok)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhUpQEl7BVUD",
        "colab_type": "text"
      },
      "source": [
        "## unzip_to_dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o30d5NkMBexH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile \n",
        "  \n",
        "def unzip_to_dir(source_list, dest_list, dry_run=False, view_contents=False):\n",
        "  for s,d in zip(source_list, dest_list):\n",
        "    # opening the zip file in READ mode \n",
        "    with ZipFile(s, 'r') as archive: \n",
        "      if dry_run:\n",
        "        print(f\"Dry run Extracting {s} to {d}\")\n",
        "        if view_contents:\n",
        "          # printing all the contents of the zip file \n",
        "          archive.printdir()\n",
        "      else:\n",
        "        # extracting all the files \n",
        "        print(f\"Extracting {s} to {d}\") \n",
        "        archive.extractall(path=d) \n",
        "      print('Done!') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywPFO-97E0bK",
        "colab_type": "text"
      },
      "source": [
        "## extract_string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Bua3ur0bN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "def extract_string(file_path, split, index):\n",
        "  \"\"\"\n",
        "  Get string by splitting path\n",
        "  \n",
        "  @arg file_path The file path string to split\n",
        "  @arg split Caracter to split path on\n",
        "  @index Index integer to retain\n",
        "\n",
        "  @return A string\n",
        "  \"\"\" \n",
        "  return os.path.splitext(file_path)[0].split(split)[index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVrYXgDSTip_",
        "colab_type": "text"
      },
      "source": [
        "## unchunk_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl2-6LMcTXxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unchunk coords\n",
        "# from xcube\n",
        "import json\n",
        "import os.path\n",
        "from typing import List, Sequence\n",
        "\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import zarr\n",
        "\n",
        "\n",
        "def unchunk_dataset(dataset_path: str, var_names: Sequence[str] = None, coords_only: bool = False):\n",
        "    \"\"\"\n",
        "    Unchunk dataset variables in-place.\n",
        "    :param dataset_path: Path to ZARR dataset directory.\n",
        "    :param var_names: Optional list of variable names.\n",
        "    :param coords_only: Un-chunk coordinate variables only.\n",
        "    \"\"\"\n",
        "\n",
        "    is_zarr = os.path.isfile(os.path.join(dataset_path, '.zgroup'))\n",
        "    if not is_zarr:\n",
        "        raise ValueError(f'{dataset_path!r} is not a valid Zarr directory')\n",
        "\n",
        "    with xr.open_zarr(dataset_path) as dataset:\n",
        "        if var_names is None:\n",
        "            if coords_only:\n",
        "                var_names = list(dataset.coords)\n",
        "            else:\n",
        "                var_names = list(dataset.variables)\n",
        "        else:\n",
        "            for var_name in var_names:\n",
        "                if coords_only:\n",
        "                    if var_name not in dataset.coords:\n",
        "                        raise ValueError(f'variable {var_name!r} is not a coordinate variable in {dataset_path!r}')\n",
        "                else:\n",
        "                    if var_name not in dataset.variables:\n",
        "                        raise ValueError(f'variable {var_name!r} is not a variable in {dataset_path!r}')\n",
        "\n",
        "    _unchunk_vars(dataset_path, var_names)\n",
        "\n",
        "\n",
        "def _unchunk_vars(dataset_path: str, var_names: List[str]):\n",
        "    for var_name in var_names:\n",
        "        var_path = os.path.join(dataset_path, var_name)\n",
        "\n",
        "        # Optimization: if \"shape\" and \"chunks\" are equal in ${var}/.zarray, we are done\n",
        "        var_array_info_path = os.path.join(var_path, '.zarray')\n",
        "        with open(var_array_info_path, 'r') as fp:\n",
        "            var_array_info = json.load(fp)\n",
        "            if var_array_info.get('shape') == var_array_info.get('chunks'):\n",
        "                continue\n",
        "\n",
        "        # Open array and remove chunks from the data\n",
        "        var_array = zarr.convenience.open_array(var_path, 'r+')\n",
        "        if var_array.shape != var_array.chunks:\n",
        "            # TODO (forman): Fully loading data is inefficient and dangerous for large arrays.\n",
        "            #                Instead save unchunked to temp and replace existing chunked array dir with temp.\n",
        "            # Fully load data and attrs so we no longer depend on files\n",
        "            data = np.array(var_array)\n",
        "            attributes = var_array.attrs.asdict()\n",
        "            # Save array data\n",
        "            zarr.convenience.save_array(var_path, data, chunks=False, fill_value=var_array.fill_value)\n",
        "            # zarr.convenience.save_array() does not seem save user attributes (file \".zattrs\" not written),\n",
        "            # therefore we must modify attrs explicitly:\n",
        "            var_array = zarr.convenience.open_array(var_path, 'r+')\n",
        "            var_array.attrs.update(attributes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lNDlU-IcyiZf"
      },
      "source": [
        "## write_to_remote_zarr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mCSbC4x_yiZr",
        "colab": {}
      },
      "source": [
        "import gcsfs\n",
        "import zarr\n",
        "import xarray as xr\n",
        "\n",
        "def write_to_remote_zarr(\n",
        "    ds,\n",
        "    group,\n",
        "    root,\n",
        "    unchunk_coords = True,\n",
        "    project_id = gc_project,\n",
        "    token=f\"/root/.{gc_creds}\",\n",
        "    show_tree = True\n",
        "    ):\n",
        "  \n",
        "  # Connect to GS\n",
        "  gc = gcsfs.GCSFileSystem(project=project_id, token=token)\n",
        "  store = gc.get_mapper(root, check=False, create=True)\n",
        "  \n",
        "  # consolidate metadata at root\n",
        "  zarr.consolidate_metadata(store)\n",
        "  \n",
        "  # Write to zarr group\n",
        "  ds.to_zarr(store=store, group=group, mode=\"w\", consolidated=True)\n",
        "  \n",
        "  # consolidate metadata at root\n",
        "  zarr.consolidate_metadata(store)\n",
        "  c = gc.exists(f\"{root}/.zmetadata\")\n",
        "  print(f\"{root} is consoldiated? {c}\")\n",
        "  # unchunk coordinates\n",
        "  # TODO: optimise this for remote ZARR\n",
        "  #if unchunk_coords:\n",
        "  #  unchunk_dataset(store, coords_only = True)\n",
        "  if show_tree:\n",
        "    with zarr.open_consolidated(store, mode='r') as z:\n",
        "      print(z.tree())\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1y_R6gT8btq",
        "colab_type": "text"
      },
      "source": [
        "## set_acl_to_public"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWaLaClDVSaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import subprocess\n",
        "\n",
        "# Set to asset permissions to public for https read\n",
        "def set_acl_to_public(gs_path):\n",
        "  \"\"\" \n",
        "  Set all Google Storage assets to puplic read access.\n",
        "\n",
        "  Requires GS authentication\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  gs_path str\n",
        "    The google storage path, note the \"-r\" option is used, setting the acl of all assets below this path\n",
        "  \"\"\"\n",
        "  cmd = f\"gsutil -m acl -r ch -u AllUsers:R {gs_path}\"\n",
        "  print(cmd)\n",
        "  r = subprocess.call(cmd, shell=True)\n",
        "  if r is 0:\n",
        "    print(\"Set acl(s) sucsessful\")\n",
        "  else:\n",
        "    print(\"Set acl(s) failed\")  \n",
        "\n",
        "#set_acl_to_public(\"gs://skydipper-water-quality/cloud-masks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpA8zrpla9R_",
        "colab_type": "text"
      },
      "source": [
        "## rmv_remote_zarr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUOpAwuybCFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gcsfs\n",
        "import zarr\n",
        "\n",
        "def rmv_remote_zarr(\n",
        "    group,\n",
        "    root,\n",
        "    project_id = gc_project,\n",
        "    token=f\"/root/.{gc_creds}\",\n",
        "     show_tree=False):\n",
        "  \n",
        "  # Connect to GS\n",
        "  gc = gcsfs.GCSFileSystem(project=project_id, token=token)\n",
        "  store = gc.get_mapper(root, check=False, create=True)\n",
        "  # Remove zarr group\n",
        "  print(f\"Removing {root}/{group}\")\n",
        "  zarr.storage.rmdir(store, path=group)\n",
        "  # consolidate metadata at root\n",
        "  zarr.consolidate_metadata(store)\n",
        "  with zarr.open_consolidated(store, mode='r') as z:\n",
        "    print(z.tree())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEueuszwbnIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "#rmv_remote_zarr_group('pet-minmax-monthly-era5', root = \"copernicus-climate/spain.zarr\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rXpWQ5zJhRDn"
      },
      "source": [
        "## get_size_remote_zarr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RL5Iet8thRDx",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "def get_size_remote_zarr(\n",
        "    group,\n",
        "    root):\n",
        "  \n",
        "  # Get size using gsutil\n",
        "  if group:\n",
        "    p = f\"gs://{root}/{group}\"\n",
        "  else:\n",
        "    p = f\"gs://{root}\"  \n",
        "  cmd = f\"gsutil -m du -sh {p}\"\n",
        "  print(f\"Processing: {cmd}\")\n",
        "  r = subprocess.getoutput(cmd)\n",
        "  print(r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTOD9R2KyFJ_",
        "colab_type": "text"
      },
      "source": [
        "## get_cached_remote_zarr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwZhd30U2hso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gcsfs\n",
        "import zarr\n",
        "import xarray as xr\n",
        "\n",
        "\n",
        "\n",
        "def get_cached_remote_zarr(\n",
        "    group,\n",
        "    root,\n",
        "    project_id = gc_project,\n",
        "    token=f\"/root/.{gc_creds}\"\n",
        "    ):\n",
        "  \n",
        "  # Connect to GS\n",
        "  gc = gcsfs.GCSFileSystem(project=project_id, token=token)\n",
        "  store = gc.get_mapper(root, check=False, create=True)\n",
        "  # Check zarr is consolidated\n",
        "  consolidated = gc.exists(f'{root}/.zmetadata')\n",
        "  # Cache the zarr store\n",
        "  #store = zarr.ZipStore(store, mode='r')\n",
        "  cache = zarr.LRUStoreCache(store, max_size=None)\n",
        "  # Return cached zarr group\n",
        "  return xr.open_zarr(cache, group=group, consolidated=consolidated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0HtSASYIj0N",
        "colab_type": "text"
      },
      "source": [
        "## cold_spell_frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzuIh6P5v_XY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Coldsnap events\n",
        "from xclim.indices import run_length as rl\n",
        "\n",
        "def cold_spell_frequency(da_tasmin, da_tasmin_reference, quantile=0.05, windows=[2,4,5], names=['warnings', 'alerts', 'alarms'], freq='MS'):\n",
        "  # Create quantile for reference array\n",
        "  da_quantile = da_tasmin_reference.quantile(quantile, dim=[\"time\"])\n",
        "  syear = da_tasmin_reference.time.dt.strftime('%Y').values[0]\n",
        "  eyear = da_tasmin_reference.time.dt.strftime('%Y').values[-1]\n",
        "  # Create bool array\n",
        "  ba = da_tasmin < da_quantile\n",
        "  # Resample to freq\n",
        "  group = ba.resample(time='MS')\n",
        "  # Calculate sum of events per frequency per window\n",
        "  da_list = [group.map(rl.windowed_run_events, window=window, dim=\"time\") for window in windows]\n",
        "  da_dict = dict(zip(names, da_list))\n",
        "  # Rename and add metadata\n",
        "  if freq == 'MS': freq = 'Monthly'\n",
        "  if freq == 'YS': freq = 'Yearly'\n",
        "  for name, window in zip(names, windows):\n",
        "    attrs = {\n",
        "        'units':\"\",\n",
        "        'standard_name': \"coldsnap_events\",\n",
        "        'long_name': f\"Number of cold snap events (Tmin < Tmin_q{quantile} for >= {window} days)\",\n",
        "        'description': f\"{freq} number of cold snap events. \"\n",
        "        \"An event occurs when the minimum daily \"\n",
        "        \"temperature is lower than a specific threshold per cell : \"\n",
        "        f\"(Tmin < Tmin_q{quantile}) \"\n",
        "        f\"over a minimum number of days ({window}), where \"\n",
        "        f\"Tmin_q{quantile} is calculated for the reference time-interval \"\n",
        "        f\"{syear}--{eyear}.\",\n",
        "    }\n",
        "    da_dict[name].attrs = attrs\n",
        "    da_dict[name].name = f\"coldsnap_{name}\"\n",
        "  # Combine into a dataset\n",
        "  return xr.merge(list(da_dict.values())).drop('quantile')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gO9rkXZqL1H5"
      },
      "source": [
        "## heat_wave_frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-YsgMywXL1IB",
        "colab": {}
      },
      "source": [
        "# Heatwave events\n",
        "from xclim.indices import run_length as rl\n",
        "\n",
        "def heat_wave_frequency(da_tasmax,\n",
        "                        da_tasmin,\n",
        "                        ds_reference,\n",
        "                        quantiles={'tasmin':0.90, 'tasmax':0.95},\n",
        "                        windows=[2,4,5],\n",
        "                        names=['warnings', 'alerts', 'alarms'],\n",
        "                        freq='MS'):\n",
        "  # Create quantiles for reference array\n",
        "  da_tasmax_quantile = ds_reference.tasmax.quantile(quantiles['tasmax'], dim=[\"time\"])\n",
        "  da_tasmin_quantile = ds_reference.tasmin.quantile(quantiles['tasmin'], dim=[\"time\"])\n",
        "  syear = ds_reference.time.dt.strftime('%Y').values[0]\n",
        "  eyear = ds_reference.time.dt.strftime('%Y').values[-1]\n",
        "  # Create bool array\n",
        "  ba = (da_tasmin > da_tasmin_quantile) & (da_tasmax > da_tasmax_quantile)\n",
        "  # Resample to freq\n",
        "  group = ba.resample(time='MS')\n",
        "  # Calculate sum of events per frequency per window\n",
        "  da_list = [group.map(rl.windowed_run_events, window=window, dim=\"time\") for window in windows]\n",
        "  da_dict = dict(zip(names, da_list))\n",
        "  # Rename and add metadata\n",
        "  if freq == 'MS': freq = 'Monthly'\n",
        "  if freq == 'YS': freq = 'Yearly'\n",
        "  for name, window in zip(names, windows):\n",
        "    attrs = {\n",
        "        'units':\"\",\n",
        "        'standard_name': \"heat_wave_events\",\n",
        "        'long_name': f\"Number of heat wave events (Tmin > Tmin_q{quantiles['tasmin']} & Tmax > Tmax_q{quantiles['tasmax']} for >= {window} days)\",\n",
        "        'description': f\"{freq} number of heat wave events. \"\n",
        "        \"An event occurs when the minimum and maximum daily \"\n",
        "        \"temperatures are higher than specific thresholds per cell : \"\n",
        "        f\"(Tmin > Tmin_q{quantiles['tasmin']}) & (Tmax > Tmax_q{quantiles['tasmax']}) \"\n",
        "        f\"over a minimum number of days ({window}), where \"\n",
        "        f\"Tmin_q{quantiles['tasmin']} and Tmax_q{quantiles['tasmax']} are calculated for the reference time-interval \"\n",
        "        f\"{syear}--{eyear}.\",\n",
        "    }\n",
        "    da_dict[name].attrs = attrs\n",
        "    da_dict[name].name = f\"heatwave_{name}\"\n",
        "  # Combine into a dataset\n",
        "  return xr.merge(list(da_dict.values()))#.drop('quantile') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBPS3DALtie0",
        "colab_type": "text"
      },
      "source": [
        "# Processing\n",
        "\n",
        "Data processing organised into sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMBZ7SwItthj",
        "colab_type": "text"
      },
      "source": [
        "## Get datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppQOQKoG9Pe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creat directory structure\n",
        "ds_dir = \"dataset\"\n",
        "mkdirs([ds_dir])\n",
        "# Add pet to process hourly pet\n",
        "# \"pet\"\n",
        "data_dirs = [\"coldsnaps\", \"heatwaves\", \"tasmax\", \"tasmin\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjX2e51nXPfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get datasets\n",
        "dest_list = [f\"{ds_dir}\" for p in data_dirs]\n",
        "source_list = [f\"{gcs_prefix}/{p}\" for p in data_dirs]\n",
        "copy_gcs(source_list, dest_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3mXDqSsCC3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "# Unzip datasets\n",
        "source_paths = [glob.glob(f\"/content/{ds_dir}/{d}/*.zip\") for d in data_dirs]\n",
        "#print(source_paths)\n",
        "dest_paths = [[f\"/content/{ds_dir}/{d}\" for s in sp] for d, sp in zip(data_dirs, source_paths)]\n",
        "#print(dest_paths)\n",
        "[unzip_to_dir(sp, dp, dry_run=False) for sp, dp in zip(source_paths, dest_paths)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBWHD5khL9VP",
        "colab_type": "text"
      },
      "source": [
        "## Combine datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsS9OibJWwGL",
        "colab_type": "text"
      },
      "source": [
        "### PET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kwYlSNrtwmR",
        "colab_type": "text"
      },
      "source": [
        "#### Historical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQLKpRntSrYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%time\n",
        "# Here we are only reading the metadata!\n",
        "ds_pet = get_cached_remote_zarr(\"pet-hourly-era5\", 'copernicus-climate/spain.zarr')\n",
        "ds_pet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I7T2sIHjsjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Historical monthly\n",
        "# note no calculations actually done!\n",
        "# note resolution is 0.25!\n",
        "# female, adult, met = 80.0, clo = 0.9\n",
        "\n",
        "# Monthly max\n",
        "ds_pet_max = ds_pet.sel(met=80.0, clo=0.9, gender='female', age_cat='adult').resample(time='MS').max()\n",
        "ds_pet_max = ds_pet_max.rename_vars({'pet':'petmax'})\n",
        "\n",
        "# Update metadata\n",
        "ds_pet_max.petmax.attrs = {\n",
        "   'long_name' : 'Monthly maximum Physiologically Equivalent Temperature (PET)',\n",
        "   'standard_name' : 'maximum_physiologically_equivalent_temperature',\n",
        "   'units' : '1' \n",
        "}\n",
        "# Monthly min\n",
        "ds_pet_min = ds_pet.sel(met=80.0, clo=0.9, gender='female', age_cat='adult').resample(time='MS').min()\n",
        "ds_pet_min = ds_pet_min.rename_vars({'pet':'petmin'})\n",
        "\n",
        "# Update metadata\n",
        "ds_pet_min.petmin.attrs = {\n",
        "   'long_name' : 'Monthly minimum Physiologically Equivalent Temperature (PET)',\n",
        "   'standard_name' : 'minimum_physiologically_equivalent_temperature',\n",
        "   'units' : '1' \n",
        "}\n",
        "\n",
        "ds_peth = xr.merge([ds_pet_max, ds_pet_min])\n",
        "ds_peth = ds_peth.drop(['age_cat', 'met', 'gender', 'clo'])\n",
        "ds_peth\n",
        "\n",
        "# change direction of lat\n",
        "ds_peth = ds_peth.sortby(['time', 'lon', 'lat'])\n",
        "ds_peth "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYLq75exjFSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ds_peth.petmax.max('time').plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tRfjXpzZGvbt"
      },
      "source": [
        "### Daily maximum temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MstZ8WqXGvbz"
      },
      "source": [
        "#### Historical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C3tMTcgwGkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Here we are only reading the metadata!\n",
        "his = get_cached_remote_zarr(\"reanalysis-era5-land\", 'copernicus-climate/spain.zarr')\n",
        "print(his)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMqH2mPN0gLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Monthly max tasmax\n",
        "ds_tasmaxh = his.tasmax.resample(time='MS').max().sortby(['time', 'lon', 'lat'])\n",
        "print(ds_tasmaxh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zEU8QtKoGvcJ"
      },
      "source": [
        "#### Longterm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l8WnPcl7GvcM",
        "colab": {}
      },
      "source": [
        "# Define file name options\n",
        "models = ['ACCESS1-0', 'BNU-ESM', 'MPI-ESM-LR', 'MPI-ESM-MR', 'NorESM1-M']\n",
        "experiments = ['rcp85', 'rcp45']\n",
        "times = ['2020', '2030', '2040', '2050', '2060', '2070', '2080', '2090']\n",
        "\n",
        "# Combine datasets\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "def prep_dataset(fp, model, experiment, time):\n",
        "  ds = xr.open_dataset(fp)\n",
        "  ds['model'] = [model]\n",
        "  ds['time'] = [time]\n",
        "  ds['experiment'] = [experiment]\n",
        "  ds = ds.set_coords(['time', 'longitude', 'latitude', 'experiment', 'model'])\n",
        "  #ds[var] = ds[var].dt.days\n",
        "  #ds = ds.rename_vars({var:f\"coldsnap_{var}\"})\n",
        "  return ds\n",
        "\n",
        "ds_list = list()\n",
        "for e in experiments:\n",
        "  for m in models:\n",
        "    for t in times:\n",
        "      fp = f\"/content/dataset/tasmax/tasmax_{m}_{e}_Spain_{t}.nc\"\n",
        "      model = m\n",
        "      experiment = e\n",
        "      time = pd.to_datetime(t)\n",
        "      ds_list.append(prep_dataset(fp, model, experiment, time))\n",
        "\n",
        "ds_tasmaxlt = xr.combine_by_coords(ds_list)\n",
        "ds_tasmaxlt = ds_tasmaxlt.set_index(lat='latitude', lon='longitude', append=True)\n",
        "\n",
        "# Create attributes\n",
        "attrs = {\n",
        "    'description' : 'Daily maximum near-surface (usually 2m) air temperature.',\n",
        "    'history' : 'Hourly surface temperature (tas) values, derived from the ERA5-Land reanalysis, were converted to daily maximum (tasmax) values for the reference period (1986-2005). Projected future temperature anomalies for time-intervals of 20 years (e.g., 2010-2030, 2020-2040), derived from the results of 2 climate experiements (rcp45, and rcp85) 5 models carried out in the CMIP5 intercomparison, were added to the reference period.',\n",
        "    'source' : 'ER5-Land and CIMP5 available from the Copernicus Climate Data Store.'\n",
        "}\n",
        "ds_tasmaxlt.attrs = attrs\n",
        "ds_tasmaxlt.tasmax.attrs = {\n",
        "    'long_name' : 'Daily Maximum Near-Surface Air Temperature',\n",
        "    'units' : 'K',\n",
        "    'standard_name' : 'daily_maximum_air_temperature',\n",
        "    'type' : 'real',\n",
        "    'Conventions' :'CF-1.7',\n",
        "    'source' :'ECMWF',\n",
        "    'institution' : 'European Centre for Medium-Range Weather Forecasts'\n",
        "    }\n",
        "\n",
        "# Chunk\n",
        "ds_tasmaxlt = ds_tasmaxlt.sortby(['experiment', 'model', 'time', 'lon', 'lat']).chunk({'experiment':1})\n",
        "ds_tasmaxlt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pHMXBgxmGvcZ"
      },
      "source": [
        "#### Seasonal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OVuHRfkCGvca",
        "colab": {}
      },
      "source": [
        "# Define file name options\n",
        "models = ['cmcc_3', 'dwd_2', 'ecmwf_5', 'meteo_france_7', 'ukmo_14', 'ncep_2']\n",
        "times = ['2020_02', '2020_03', '2020_04', '2020_05', '2020_06', '2020_07']\n",
        "\n",
        "# Combine datasets\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "def prep_dataset(fp, model, time):\n",
        "  ds = xr.open_dataset(fp)\n",
        "  ds['model'] = [model]\n",
        "  ds['time'] = [time]\n",
        "  ds = ds.set_coords(['time', 'longitude', 'latitude', 'model'])\n",
        "  return ds\n",
        "\n",
        "ds_list = list()\n",
        "for m in models:\n",
        "    for t in times:\n",
        "      fp = f\"/content/dataset/tasmax/tasmax_seasonal_02_{t}_{m}.nc\"\n",
        "      model = m\n",
        "      ts = t.split('_')\n",
        "      time = pd.to_datetime(f\"{ts[0]}-{ts[1]}-01\")\n",
        "      ds_list.append(prep_dataset(fp, model, time))\n",
        "\n",
        "ds_tasmaxse = xr.combine_by_coords(ds_list)\n",
        "ds_tasmaxse = ds_tasmaxse.set_index(lat='latitude', lon='longitude', append=True)\n",
        "\n",
        "# Create attributes\n",
        "attrs = {\n",
        "    'description' : 'Daily maximum near-surface (usually 2m) air temperature.',\n",
        "    'history' : '',\n",
        "    'source' : 'Derived from `Seasonal forecast daily data on single levels from 2017 to present` and ERA5-Land available in the Copernicus Climate Data Store'\n",
        "}\n",
        "ds_tasmaxse.attrs = attrs\n",
        "ds_tasmaxse.tasmax.attrs = {\n",
        "    'long_name' : 'Daily Maximum Near-Surface Air Temperature',\n",
        "    'units' : 'K',\n",
        "    'standard_name' : 'daily_maximum_air_temperature',\n",
        "    'type' : 'real',\n",
        "    'Conventions' :'CF-1.7',\n",
        "    'source' :'ECMWF',\n",
        "    'institution' : 'European Centre for Medium-Range Weather Forecasts'\n",
        "    }\n",
        "# Chunk\n",
        "ds_tasmaxse = ds_tasmaxse.sortby(['model', 'time', 'lon', 'lat']).chunk({'time':-1})\n",
        "ds_tasmaxse\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3hIilgETLQPp"
      },
      "source": [
        "### Daily minimum temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C9nWbmv1LQPx"
      },
      "source": [
        "#### Historical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fED9jgXd0QJD",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Here we are only reading the metadata!\n",
        "his = get_cached_remote_zarr(\"reanalysis-era5-land\", 'copernicus-climate/spain.zarr')\n",
        "print(his)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MYRRGeSU0QJf",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Monthly min tasmin\n",
        "ds_tasminh = his.tasmin.resample(time='MS').min().sortby(['time', 'lon', 'lat'])\n",
        "print(ds_tasminh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5OOyrLrJLQQF"
      },
      "source": [
        "#### Longterm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VQwGitFZLQQI",
        "colab": {}
      },
      "source": [
        "# Define file name options\n",
        "models = ['ACCESS1-0', 'BNU-ESM', 'MPI-ESM-LR', 'MPI-ESM-MR', 'NorESM1-M']\n",
        "experiments = ['rcp85', 'rcp45']\n",
        "times = ['2020', '2030', '2040', '2050', '2060', '2070', '2080', '2090']\n",
        "\n",
        "# Combine datasets\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "def prep_dataset(fp, model, experiment, time):\n",
        "  ds = xr.open_dataset(fp)\n",
        "  ds['model'] = [model]\n",
        "  ds['time'] = [time]\n",
        "  ds['experiment'] = [experiment]\n",
        "  ds = ds.set_coords(['time', 'longitude', 'latitude', 'experiment', 'model'])\n",
        "  #ds[var] = ds[var].dt.days\n",
        "  #ds = ds.rename_vars({var:f\"coldsnap_{var}\"})\n",
        "  return ds\n",
        "\n",
        "ds_list = list()\n",
        "for e in experiments:\n",
        "  for m in models:\n",
        "    for t in times:\n",
        "      fp = f\"/content/dataset/tasmin/tasmin_{m}_{e}_Spain_{t}.nc\"\n",
        "      model = m\n",
        "      experiment = e\n",
        "      time = pd.to_datetime(t)\n",
        "      ds_list.append(prep_dataset(fp, model, experiment, time))\n",
        "\n",
        "ds_tasminlt = xr.combine_by_coords(ds_list)\n",
        "ds_tasminlt = ds_tasminlt.set_index(lat='latitude', lon='longitude', append=True)\n",
        "\n",
        "# Create attributes\n",
        "attrs = {\n",
        "    'description' : 'Daily minimum near-surface (usually 2m) air temperature.',\n",
        "    'history' : 'Hourly surface temperature (tas) values, derived from the ERA5-Land reanalysis, were converted to daily minimum (tasmin) values for the reference period (1986-2005). Projected future temperature anomalies for time-intervals of 20 years (e.g., 2010-2030, 2020-2040), derived from the results of 2 climate experiements (rcp45, and rcp85) 5 models carried out in the CMIP5 intercomparison, were added to the reference period.',\n",
        "    'source' : 'ER5-Land and CIMP5 available from the Copernicus Climate Data Store.'\n",
        "}\n",
        "ds_tasminlt.attrs = attrs\n",
        "ds_tasminlt.tasmin.attrs = {\n",
        "    'long_name' : 'Daily Minimum Near-Surface Air Temperature',\n",
        "    'units' : 'K',\n",
        "    'standard_name' : 'daily_minimum_air_temperature',\n",
        "    'type' : 'real',\n",
        "    'Conventions' :'CF-1.7',\n",
        "    'source' :'ECMWF',\n",
        "    'institution' : 'European Centre for Medium-Range Weather Forecasts'\n",
        "    }\n",
        "\n",
        "# Chunk\n",
        "ds_tasminlt = ds_tasminlt.sortby(['experiment', 'model', 'time', 'lon', 'lat']).chunk({'experiment':1})\n",
        "ds_tasminlt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0_RDae2eLQQW"
      },
      "source": [
        "#### Seasonal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c_PAtTFpLQQZ",
        "colab": {}
      },
      "source": [
        "# Define file name options\n",
        "models = ['cmcc_3', 'dwd_2', 'ecmwf_5', 'meteo_france_7', 'ukmo_14', 'ncep_2']\n",
        "times = ['2020_02', '2020_03', '2020_04', '2020_05', '2020_06', '2020_07']\n",
        "\n",
        "# Combine datasets\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "def prep_dataset(fp, model, time):\n",
        "  ds = xr.open_dataset(fp)\n",
        "  ds['model'] = [model]\n",
        "  ds['time'] = [time]\n",
        "  ds = ds.set_coords(['time', 'longitude', 'latitude', 'model'])\n",
        "  return ds\n",
        "\n",
        "ds_list = list()\n",
        "for m in models:\n",
        "    for t in times:\n",
        "      fp = f\"/content/dataset/tasmin/tasmin_seasonal_02_{t}_{m}.nc\"\n",
        "      model = m\n",
        "      ts = t.split('_')\n",
        "      time = pd.to_datetime(f\"{ts[0]}-{ts[1]}-01\")\n",
        "      ds_list.append(prep_dataset(fp, model, time))\n",
        "\n",
        "ds_tasminse = xr.combine_by_coords(ds_list)\n",
        "ds_tasminse = ds_tasminse.set_index(lat='latitude', lon='longitude', append=True)\n",
        "\n",
        "# Create attributes\n",
        "attrs = {\n",
        "    'description' : 'Daily minimum near-surface (usually 2m) air temperature.',\n",
        "    'history' : '',\n",
        "    'source' : 'Derived from `Seasonal forecast daily data on single levels from 2017 to present` and ERA5-Land available in the Copernicus Climate Data Store'\n",
        "}\n",
        "ds_tasminse.attrs = attrs\n",
        "ds_tasminse.tasmin.attrs = {\n",
        "    'long_name' : 'Daily Minimum Near-Surface Air Temperature',\n",
        "    'units' : 'K',\n",
        "    'standard_name' : 'daily_minimum_air_temperature',\n",
        "    'type' : 'real',\n",
        "    'Conventions' :'CF-1.7',\n",
        "    'source' :'ECMWF',\n",
        "    'institution' : 'European Centre for Medium-Range Weather Forecasts'\n",
        "    }\n",
        "# Chunk\n",
        "ds_tasminse = ds_tasminse.sortby(['model', 'time', 'lon', 'lat']).chunk({'time':-1})\n",
        "ds_tasminse\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFqoQ59hN7bT",
        "colab_type": "text"
      },
      "source": [
        "### Cold snaps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yph5a4aoQn_5",
        "colab_type": "text"
      },
      "source": [
        "#### Historical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RknmGWK5Z9cH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Here we are only reading the metadata!\n",
        "his = get_cached_remote_zarr(\"reanalysis-era5-land\", 'copernicus-climate/spain.zarr')\n",
        "print(his)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5G2BfC5JFnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Calculate colde snaps using 0.05 quantile per cell for time interval 1981--2019\n",
        "da_tasmin =  his.tasmin.chunk({'time':-1})\n",
        "da_tasmin_reference =  his.tasmin.chunk({'time':-1})\n",
        "ds_csh = cold_spell_frequency(da_tasmin, da_tasmin_reference, quantile=0.05, windows=[2,4,5], names=['warnings', 'alerts', 'alarms'], freq='MS')\n",
        "ds_csh = ds_csh.sortby(['time', 'lon', 'lat'])\n",
        "print(ds_csh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH28BX1WtlZH",
        "colab_type": "text"
      },
      "source": [
        "#### Longterm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bz14pIXvW3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define file name options\n",
        "models = ['ACCESS1-0', 'BNU-ESM', 'MPI-ESM-LR', 'MPI-ESM-MR', 'NorESM1-M']\n",
        "experiments = ['rcp85', 'rcp45']\n",
        "risk_longterm = ['Alarms', 'Alerts', 'Warnings']\n",
        "times = ['2020', '2030', '2040', '2050', '2060', '2070', '2080', '2090']\n",
        "\n",
        "# Combine datasets\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "def prep_dataset(fp, var, model, experiment, time):\n",
        "  ds = xr.open_dataset(fp)\n",
        "  ds['model'] = [model]\n",
        "  ds['time'] = [time]\n",
        "  ds['experiment'] = [experiment]\n",
        "  ds = ds.set_coords(['time', 'longitude', 'latitude', 'experiment', 'model'])\n",
        "  #ds = ds.set_index(time='time', lat='latitude', lon='longitude', experiment='experiment', model='model')\n",
        "  ds[var] = ds[var].dt.days\n",
        "  ds = ds.rename_vars({var:f\"coldsnap_{var}\"})\n",
        "  return ds\n",
        "\n",
        "ds_list = list()\n",
        "for v in risk_longterm:\n",
        "  for e in experiments:\n",
        "    for m in models:\n",
        "      for t in times:\n",
        "        fp = f\"/content/dataset/coldsnaps/coldSpell{v}_{m}_{e}_Spain_{t}.nc\"\n",
        "        var = v.lower()\n",
        "        model = m\n",
        "        experiment = e\n",
        "        time = pd.to_datetime(t)\n",
        "        ds_list.append(prep_dataset(fp, var, model, experiment, time))\n",
        "\n",
        "ds_cslt = xr.combine_by_coords(ds_list)\n",
        "ds_cslt = ds_cslt.set_index(lat='latitude', lon='longitude', append=True)\n",
        "\n",
        "# Create attributes\n",
        "attrs = {\n",
        "    'description' : 'Number of Cold snap events, defined as a period of consecutive days when the minimum daily temperature (tasmin) is lower than the 0.05 quantile observed in the corresponding grid cell of the reference period (1986-2005). A Warning event represents this condition for 1 to 2 consecutive days, an Alert event is 3 to 4 days, and an Alarm event lasts more than 4 days.',\n",
        "    'history' : 'Hourly surface temperature (tas) values, derived from the ERA5-Land reanalysis, were converted to minimum daily (tasmin) values for the reference period (1986-2005), and the 0.05 quantile (tasmin_0.05) calculated per cell. Projected future temperature anomalies for future time-intervals of 20 years (e.g., 2010-2030, 2020-2040), derived from the results of 2 climate experiements (rcp45, and rcp85) 5 models carried out in the CMIP5 intercomparison, were added to the reference period, and the number of events were tasmin was less than tasmin_0.05 for 2, 4 and 5 or more consecutive days  was calculated per cell.',\n",
        "    'source' : 'Derived from CMIP5 and ER5-Land available in the Copernicus Climate Data Store'\n",
        "}\n",
        "ds_cslt.attrs = attrs\n",
        "ds_cslt.coldsnap_alarms.attrs = {'longname' : \"number_of_cold_snap_alarm_events_in_time_period\", 'units' : \"number_of_events_per_20_years\", 'description': 'Defined as a period of more than 4 consecutive days when the minimum daily temperature (tasmin) is lower than the 0.05 quantile observed in the corresponding grid cell of the reference period (1986-2005).'}\n",
        "ds_cslt.coldsnap_alerts.attrs = {'longname' : \"number_of_cold_snap_alert_events_in_time_period\", 'units' : \"number_of_events_per_20_years\", 'description': 'Defined as a period of 3-4 consecutive days when the minimum daily temperature (tasmin) is lower than the 0.05 quantile observed in the corresponding grid cell of the reference period (1986-2005).'} \n",
        "ds_cslt.coldsnap_warnings.attrs = {'longname' : \"number_of_cold_snap_warning_events_in_time_period\", 'units' : \"number_of_events_per_20_years\", 'description': 'Defined as a period of 1-2 consecutive days when the minimum daily temperature (tasmin) is lower than the 0.05 quantile observed in the corresponding grid cell of the reference period (1986-2005).'}\n",
        "\n",
        "# Chunk\n",
        "ds_cslt = ds_cslt.sortby(['experiment', 'model', 'time', 'lon', 'lat']).chunk({'experiment':1})\n",
        "ds_cslt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1h62W90ymErK"
      },
      "source": [
        "#### Seasonal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UPm-hM_YmErl",
        "colab": {}
      },
      "source": [
        "# Define file name options\n",
        "models = ['cmcc_3', 'dwd_2', 'ecmwf_5', 'meteo_france_7', 'ukmo_14', 'ncep_2']\n",
        "risk_longterm = ['alarm', 'alert', 'warning']\n",
        "times = ['2020_02', '2020_03', '2020_04', '2020_05', '2020_06', '2020_07']\n",
        "\n",
        "# Combine datasets\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "def prep_dataset(fp, var, model, time):\n",
        "  ds = xr.open_dataset(fp)\n",
        "  ds['model'] = [model]\n",
        "  ds['time'] = [time]\n",
        "  ds = ds.set_coords(['time', 'longitude', 'latitude', 'model'])\n",
        "  var = f\"{var}s\"\n",
        "  ds[var] = ds[var].dt.days\n",
        "  ds = ds.rename_vars({var:f\"coldsnap_{var}\"})\n",
        "  return ds\n",
        "\n",
        "ds_list = list()\n",
        "for v in risk_longterm:\n",
        "    for m in models:\n",
        "      for t in times:\n",
        "        fp = f\"/content/dataset/coldsnaps/coldspell_{v}_seasonal_02_{t}_{m}.nc\"\n",
        "        var = v.lower()\n",
        "        model = m\n",
        "        ts = t.split('_')\n",
        "        time = pd.to_datetime(f\"{ts[0]}-{ts[1]}-01\")\n",
        "        ds_list.append(prep_dataset(fp, var, model, time))\n",
        "\n",
        "ds_csse = xr.combine_by_coords(ds_list)\n",
        "ds_csse = ds_csse.set_index(lat='latitude', lon='longitude', append=True)\n",
        "\n",
        "# Create attributes\n",
        "attrs = {\n",
        "    'description' : 'Number of Cold snap events, defined as a period of consecutive days when the minimum daily temperature (tasmin) is lower than the 0.05 quantile observed in the corresponding grid cell of the reference period (1986-2005). A Warning event represents this condition for 1 to 2 consecutive days, an Alert event is 3 to 4 days, and an Alarm event lasts more than 4 days.',\n",
        "    'history' : 'Hourly surface temperature (tas) values, derived from the ERA5-Land reanalysis, were converted to minimum daily (tasmin) values for the reference period (1986-2005), and the 0.05 quantile (tasmin_0.05) calculated per cell. Projected future temperature anomalies for future time-intervals of 20 years (e.g., 2010-2030, 2020-2040), derived from the results of 2 climate experiements (rcp45, and rcp85) 5 models carried out in the CMIP5 intercomparison, were added to the reference period, and the number of events were tasmin was less than tasmin_0.05 for 2, 4 and 5 or more consecutive days  was calculated per cell.',\n",
        "    'source' : 'Derived from `Seasonal forecast daily data on single levels from 2017 to present` and ERA5-Land available in the Copernicus Climate Data Store'\n",
        "}\n",
        "ds_csse.attrs = attrs\n",
        "ds_csse.coldsnap_alarms.attrs = {'longname' : \"number_of_cold_snap_alarm_events_in_time_period\", 'units' : \"number_of_events_per_month\", 'description': 'Defined as a period of more than 4 consecutive days when the minimum daily temperature (tasmin) is lower than the 0.05 quantile observed in the corresponding grid cell of the reference period (1986-2005).'}\n",
        "ds_csse.coldsnap_alerts.attrs = {'longname' : \"number_of_cold_snap_alert_events_in_time_period\", 'units' : \"number_of_events_per_month\", 'description': 'Defined as a period of 3-4 consecutive days when the minimum daily temperature (tasmin) is lower than the 0.05 quantile observed in the corresponding grid cell of the reference period (1986-2005).'} \n",
        "ds_csse.coldsnap_warnings.attrs = {'longname' : \"number_of_cold_snap_warning_events_in_time_period\", 'units' : \"number_of_events_per_month\", 'description': 'Defined as a period of 1-2 consecutive days when the minimum daily temperature (tasmin) is lower than the 0.05 quantile observed in the corresponding grid cell of the reference period (1986-2005).'}\n",
        "\n",
        "# Chunk\n",
        "ds_csse = ds_csse.sortby(['model', 'time', 'lon', 'lat']).chunk({'time':-1})\n",
        "ds_csse\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WyXcTvEUB8yX"
      },
      "source": [
        "### Heatwaves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7ZhZeMQiOsYy"
      },
      "source": [
        "#### Historical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hhbs-ChNOsY5",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Here we are only reading the metadata!\n",
        "his = get_cached_remote_zarr(\"reanalysis-era5-land\", 'copernicus-climate/spain.zarr')\n",
        "print(his)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xRPYbHyDOsZL",
        "colab": {}
      },
      "source": [
        "#%%time\n",
        "# Calculate heatwaves using quantiles per cell for time interval 1981--2019\n",
        "da_tasmin =  his.tasmin.chunk({'time':-1})\n",
        "da_tasmax =  his.tasmax.chunk({'time':-1})\n",
        "ds_reference =  his.chunk({'time':-1})\n",
        "ds_hwh = heat_wave_frequency(da_tasmax,\n",
        "                        da_tasmin,\n",
        "                        ds_reference,\n",
        "                        quantiles={'tasmin':0.90, 'tasmax':0.95},\n",
        "                        windows=[2,4,5],\n",
        "                        names=['warnings', 'alerts', 'alarms'],\n",
        "                        freq='MS')\n",
        "ds_hwh = ds_hwh.sortby(['time', 'lon', 'lat'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DgS5x6APB8zE"
      },
      "source": [
        "#### Longterm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GW0UZNLmB8zO",
        "colab": {}
      },
      "source": [
        "# Define file name options\n",
        "models = ['ACCESS1-0', 'BNU-ESM', 'MPI-ESM-LR', 'MPI-ESM-MR', 'NorESM1-M']\n",
        "experiments = ['rcp85', 'rcp45']\n",
        "risk_longterm = ['Alarms', 'Alerts', 'Warnings2']\n",
        "times = ['2020', '2030', '2040', '2050', '2060', '2070', '2080', '2090']\n",
        "\n",
        "# Combine datasets\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "def prep_dataset(fp, var, model, experiment, time):\n",
        "  ds = xr.open_dataset(fp)\n",
        "  ds['model'] = [model]\n",
        "  ds['time'] = [time]\n",
        "  ds['experiment'] = [experiment]\n",
        "  ds = ds.set_coords(['time', 'longitude', 'latitude', 'experiment', 'model'])\n",
        "  ds[var] = ds[var].dt.days\n",
        "  ds = ds.rename_vars({var:f\"heatwave_{var}\"})\n",
        "  return ds\n",
        "\n",
        "ds_list = list()\n",
        "for v in risk_longterm:\n",
        "  for e in experiments:\n",
        "    for m in models:\n",
        "      for t in times:\n",
        "        fp = f\"/content/dataset/heatwaves/heatwaves{v}_{m}_{e}_Spain_{t}.nc\"\n",
        "        var = v.lower().split('2')[0]\n",
        "        model = m\n",
        "        experiment = e\n",
        "        time = pd.to_datetime(t)\n",
        "        ds_list.append(prep_dataset(fp, var, model, experiment, time))\n",
        "\n",
        "ds_hwlt = xr.combine_by_coords(ds_list)\n",
        "ds_hwlt = ds_hwlt.set_index(lat='latitude', lon='longitude', append=True)\n",
        "\n",
        "# Create attributes\n",
        "attrs = {\n",
        "    'description' : 'Number of Heatwave events, defined as a period of consecutive days when the daily maximum temperature (tasmax) is higher than the 0.95 quantile and daily minimum temperature (tasmin) exceeds the 0.9 quantile observed in the corresponding grid cell of the reference period (1986-2005). A Warning event represents this condition for 2 consecutive days, an Alert event is 3 to 4 days, and an Alarm event lasts more than 4 days.',\n",
        "    'history' : 'Hourly surface temperature (tas) values, derived from the ERA5-Land reanalysis, were converted to daily maximum (tasmax) and minimum (tasmin) values for the reference period (1986-2005), and the 0.95 (tasmax_0.95) and 0.9 (tasmin_0.9) quantiles for tasmax and tasmin, respectivly were calculated per cell. Projected future temperature anomalies for future time-intervals of 20 years (e.g., 2010-2030, 2020-2040), derived from the results of 2 climate experiements (rcp45, and rcp85) 5 models carried out in the CMIP5 intercomparison, were added to the reference period, and the number of events were tasmax and tasmin were more than tasmax_0.95 and tasmin_0.90 for 2, 4 and 5 or more consecutive days  was calculated per cell.',\n",
        "    'source' : 'Derived from CMIP5 and ER5-Land available in the Copernicus Climate Data Store'\n",
        "}\n",
        "ds_hwlt.attrs = attrs\n",
        "ds_hwlt.heatwave_alarms.attrs = {'longname' : \"number_of_heat_wave_alarm_events_in_time_period\", 'units' : \"number_of_events_per_20_years\", 'description': 'Defined as a period of more than 4 consecutive days when the daily maximum temperature (tasmax) is higher than the 0.95 quantile and daily minimum temperature (tasmin) exceeds the 0.9 quantile observed in the corresponding grid cell of the reference period (1986-2005).'}\n",
        "ds_hwlt.heatwave_alerts.attrs = {'longname' : \"number_of_heat_wave_alert_events_in_time_period\", 'units' : \"number_of_events_per_20_years\", 'description': 'Defined as a period of 3-4 consecutive days when the daily maximum temperature (tasmax) is higher than the 0.95 quantile and daily minimum temperature (tasmin) exceeds the 0.9 quantile observed in the corresponding grid cell of the reference period (1986-2005).'} \n",
        "ds_hwlt.heatwave_warnings.attrs = {'longname' : \"number_of_heat_wave_warning_events_in_time_period\", 'units' : \"number_of_events_per_20_years\", 'description': 'Defined as a period of 2 consecutive days when the daily maximum temperature (tasmax) is higher than the 0.95 quantile and daily minimum temperature (tasmin) exceeds the 0.9 quantile observed in the corresponding grid cell of the reference period (1986-2005).'}\n",
        "\n",
        "# Chunk\n",
        "ds_hwlt = ds_hwlt.sortby(['experiment', 'model', 'time', 'lon', 'lat']).chunk({'experiment':1})\n",
        "ds_hwlt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QfOq8idfB8zg"
      },
      "source": [
        "#### Seasonal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oiXNB4PEB8zw",
        "colab": {}
      },
      "source": [
        "# Define file name options\n",
        "models = ['cmcc_3', 'dwd_2', 'ecmwf_5', 'meteo_france_7', 'ukmo_14', 'ncep_2']\n",
        "risk_longterm = ['alarm', 'alert', 'warning']\n",
        "times = ['2020_02', '2020_03', '2020_04', '2020_05', '2020_06', '2020_07']\n",
        "\n",
        "# Combine datasets\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "def prep_dataset(fp, var, model, time):\n",
        "  ds = xr.open_dataset(fp)\n",
        "  ds['model'] = [model]\n",
        "  ds['time'] = [time]\n",
        "  ds = ds.set_coords(['time', 'longitude', 'latitude', 'model'])\n",
        "  var = f\"{var}s\"\n",
        "  ds[var] = ds[var].dt.days\n",
        "  ds = ds.rename_vars({var:f\"heatwave_{var}\"})\n",
        "  return ds\n",
        "\n",
        "ds_list = list()\n",
        "for v in risk_longterm:\n",
        "    for m in models:\n",
        "      for t in times:\n",
        "        fp = f\"/content/dataset/coldsnaps/coldspell_{v}_seasonal_02_{t}_{m}.nc\"\n",
        "        var = v.lower()\n",
        "        model = m\n",
        "        ts = t.split('_')\n",
        "        time = pd.to_datetime(f\"{ts[0]}-{ts[1]}-01\")\n",
        "        ds_list.append(prep_dataset(fp, var, model, time))\n",
        "\n",
        "ds_hwse = xr.combine_by_coords(ds_list)\n",
        "ds_hwse = ds_hwse.set_index(lat='latitude', lon='longitude', append=True)\n",
        "\n",
        "# Create attributes\n",
        "attrs = {\n",
        "    'description' : 'Number of Heatwave events, defined as a period of consecutive days when the daily maximum temperature (tasmax) is higher than the 0.95 quantile and daily minimum temperature (tasmin) exceeds the 0.9 quantile observed in the corresponding grid cell of the reference period (1986-2005). A Warning event represents this condition for 2 consecutive days, an Alert event is 3 to 4 days, and an Alarm event lasts more than 4 days.',\n",
        "    'history' : '',\n",
        "    'source' : 'Derived from `Seasonal forecast daily data on single levels from 2017 to present` and ER5-Land available in the Copernicus Climate Data Store'\n",
        "}\n",
        "ds_hwse.attrs = attrs\n",
        "ds_hwse.heatwave_alarms.attrs = {'longname' : \"number_of_heatwave_alarm_events_in_time_period\", 'units' : \"number_of_events_per_month\", 'description': 'Defined as a period of more than 4 consecutive days when the daily maximum temperature (tasmax) is higher than the 0.95 quantile and daily minimum temperature (tasmin) exceeds the 0.9 quantile observed in the corresponding grid cell of the reference period (1986-2005).'}\n",
        "ds_hwse.heatwave_alerts.attrs = {'longname' : \"number_of_heatwave_alert_events_in_time_period\", 'units' : \"number_of_events_per_month\", 'description': 'Defined as a period of more than 4 consecutive days when the daily maximum temperature (tasmax) is higher than the 0.95 quantile and daily minimum temperature (tasmin) exceeds the 0.9 quantile observed in the corresponding grid cell of the reference period (1986-2005).'} \n",
        "ds_hwse.heatwave_warnings.attrs = {'longname' : \"number_of_heatwave_warning_events_in_time_period\", 'units' : \"number_of_events_per_month\", 'description': 'Defined as a period of more than 4 consecutive days when the daily maximum temperature (tasmax) is higher than the 0.95 quantile and daily minimum temperature (tasmin) exceeds the 0.9 quantile observed in the corresponding grid cell of the reference period (1986-2005).'}\n",
        "\n",
        "# Chunk\n",
        "ds_hwse = ds_hwse.sortby(['model', 'time', 'lon', 'lat']).chunk({'time':-1})\n",
        "ds_hwse\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSjyiRhsPCZr",
        "colab_type": "text"
      },
      "source": [
        "## Merge historical, seasonal, and longterm "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fl7TkgTtPlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# interpolate PET to other grids\n",
        "ds_pethi = ds_peth.chunk({'time':-1}).interp_like(ds_tasmaxh)\n",
        "#ds_pethi.petmax.sel(time='2015-08-01').plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSAXKQlOQ827",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "historical = xr.merge([ds_tasmaxh, ds_tasminh, ds_csh, ds_hwh, ds_pethi])\n",
        "# Create attributes\n",
        "attrs = {\n",
        "    'description' : 'Daily maximum near-surface (usually 2m) air temperature, daily minimum near-surface (usually 2m) air temperature, cold snap and heatwave alarms, alerts and warnings, and maximum Physiologically Equivalent Temperature (PET).',\n",
        "    'history' : '',\n",
        "    'source' : 'Derived from ERA5, `Thermal-comfort-indices-derived-from-ERA5-reanalysis`, and ERA5-Land available in the Copernicus Climate Data Store'\n",
        "}\n",
        "historical.attrs = attrs\n",
        "historical = historical.chunk({'time':24*5})\n",
        "historical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUEgRA5KPO9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seasonal = xr.merge([ds_tasmaxse, ds_tasminse, ds_csse, ds_hwse])\n",
        "# Create attributes\n",
        "attrs = {\n",
        "    'description' : 'Daily maximum near-surface (usually 2m) air temperature, daily minimum near-surface (usually 2m) air temperature, cold snap and heatwave alarms, alerts and warnings.',\n",
        "    'history' : '',\n",
        "    'source' : 'Derived from `Seasonal forecast daily data on single levels from 2017 to present`, and ERA5-Land available in the Copernicus Climate Data Store'\n",
        "}\n",
        "seasonal.attrs = attrs\n",
        "seasonal "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WLN6_8qQEef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "longterm = xr.merge([ds_tasmaxlt, ds_tasminlt, ds_cslt, ds_hwlt])\n",
        "# Create attributes\n",
        "attrs = {\n",
        "    'description' : 'Daily maximum near-surface (usually 2m) air temperature, daily minimum near-surface (usually 2m) air temperature, cold snap and heatwave alarms, alerts and warnings.',\n",
        "    'history' : '',\n",
        "    'source' : 'Derived from ERA5-Land, and CIMP5 available in the Copernicus Climate Data Store'\n",
        "}\n",
        "longterm.attrs = attrs\n",
        "longterm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdV6nuRFZaH3",
        "colab_type": "text"
      },
      "source": [
        "### Write to Zarr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2o14vhqZd9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Create remote ZARR\n",
        "write_to_remote_zarr(historical, group = 'historical-monthly', root = \"copernicus-climate/spain.zarr\", show_tree=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVlPvDZjRt3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "write_to_remote_zarr(seasonal, group = 'future-seasonal-monthly', root = \"copernicus-climate/spain.zarr\", show_tree=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHdiUobTB9OR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "write_to_remote_zarr(longterm, group = 'future-longterm-yr10', root = \"copernicus-climate/spain.zarr\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88Ka-5Ms9fCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_size_remote_zarr(None, 'copernicus-climate/spain.zarr')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}